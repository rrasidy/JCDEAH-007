{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01a72b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "242d7ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee']\n",
      "['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge', 'cbd_congestion_fee']\n"
     ]
    }
   ],
   "source": [
    "df_yellow = pd.read_parquet(r\"C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet\")\n",
    "print(df_yellow.columns.tolist())\n",
    "\n",
    "df_green = pd.read_parquet(r\"C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet\")\n",
    "print(df_green.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "657fc9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVEL = logging.INFO\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"data_extraction_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1553123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCES = [\n",
    "    {\n",
    "        \"name\": \"yellow_trip\",\n",
    "        \"path\": r\"C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet\",  # ganti dengan path sebenarnya\n",
    "        \"format\": \"parquet\",                        # atau \"csv\"\n",
    "        \"date_column\": \"tpep_pickup_datetime\",           # ganti sesuai kolom di file\n",
    "        \"target_table\": \"public.yellow_trip\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"green_trip\",\n",
    "        \"path\": r\"C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet\",   # ganti dengan path sebenarnya\n",
    "        \"format\": \"parquet\",\n",
    "        \"date_column\": \"lpep_pickup_datetime\",           # ganti sesuai kolom di file\n",
    "        \"target_table\": \"public.green_trip\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Konfigurasi koneksi PostgreSQL (contoh) ---\n",
    "# Format: postgresql://username:password@host:port/database\n",
    "POSTGRES_CONN_STR = os.getenv(\n",
    "    \"POSTGRES_CONN_STR\",\n",
    "    \"postgresql://postgres:obet@127.0.0.1:5432/my_database\"  # ganti\n",
    ")\n",
    "\n",
    "USE_BIGQUERY = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3163100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def get_postgres_engine(conn_str: str):\n",
    "    try:\n",
    "        engine = create_engine(conn_str)\n",
    "        # Tes koneksi ringan\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        logger.info(\"Koneksi ke PostgreSQL berhasil.\")\n",
    "        return engine\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(f\"Gagal konek ke PostgreSQL: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_source_file(path: str, file_format: str) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        logger.error(f\"File tidak ditemukan: {path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if file_format.lower() == \"parquet\":\n",
    "            df = pd.read_parquet(path)\n",
    "        elif file_format.lower() == \"csv\":\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            logger.error(f\"Format file tidak didukung: {file_format}\")\n",
    "            return None\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            logger.warning(f\"Dataframe kosong dari file: {path}\")\n",
    "            return None\n",
    "\n",
    "        logger.info(f\"Berhasil baca file {path} dengan {len(df)} baris.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saat membaca file {path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataframe(\n",
    "    df: Optional[pd.DataFrame],\n",
    "    required_columns: List[str],\n",
    "    source_name: str\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Validasi dasar dataframe.\"\"\"\n",
    "    if df is None:\n",
    "        logger.error(f\"[{source_name}] Dataframe adalah None.\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        logger.error(f\"[{source_name}] Dataframe kosong.\")\n",
    "        return None\n",
    "\n",
    "    missing_cols = [c for c in required_columns if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"[{source_name}] Kolom hilang: {missing_cols}\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_by_period(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    run_date: datetime,\n",
    "    mode: str = \"monthly_first_day\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    if date_col not in df.columns:\n",
    "        raise ValueError(f\"Kolom tanggal '{date_col}' tidak ditemukan di dataframe.\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Pastikan kolom bertipe datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[date_col])\n",
    "\n",
    "    if mode == \"all\":\n",
    "        return df\n",
    "\n",
    "    if mode == \"daily\":\n",
    "        mask = df[date_col].dt.date == run_date.date()\n",
    "    elif mode == \"weekly\":\n",
    "        mask = (\n",
    "            (df[date_col].dt.isocalendar().year == run_date.isocalendar().year) &\n",
    "            (df[date_col].dt.isocalendar().week == run_date.isocalendar().week)\n",
    "        )\n",
    "    elif mode == \"monthly\":\n",
    "        mask = (\n",
    "            (df[date_col].dt.year == run_date.year) &\n",
    "            (df[date_col].dt.month == run_date.month)\n",
    "        )\n",
    "    elif mode == \"monthly_first_day\":\n",
    "        # Hanya baris yang tanggalnya = 1 di bulan yang sama\n",
    "        mask = (\n",
    "            (df[date_col].dt.year == run_date.year) &\n",
    "            (df[date_col].dt.month == run_date.month) &\n",
    "            (df[date_col].dt.day == 1)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Mode filter tidak dikenal: {mode}\")\n",
    "\n",
    "    filtered = df[mask]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Filter mode='{mode}' untuk tanggal run {run_date.date()} \"\n",
    "        f\"menghasilkan {len(filtered)} baris dari {len(df)} baris.\"\n",
    "    )\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e62e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgres(\n",
    "    df: pd.DataFrame,\n",
    "    engine,\n",
    "    table_name: str,\n",
    "    if_exists: str = \"append\",\n",
    "    chunksize: int = 10_000\n",
    "):\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"Tidak ada data yang disimpan ke PostgreSQL untuk tabel {table_name}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            name=table_name.split(\".\")[-1],    # jika \"schema.table\"\n",
    "            schema=table_name.split(\".\")[0] if \".\" in table_name else None,\n",
    "            con=engine,\n",
    "            if_exists=if_exists,\n",
    "            index=False,\n",
    "            chunksize=chunksize\n",
    "        )\n",
    "        logger.info(f\"Berhasil simpan {len(df)} baris ke PostgreSQL tabel {table_name}.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(f\"Error saat menyimpan ke PostgreSQL ({table_name}): {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error umum saat menyimpan ke PostgreSQL ({table_name}): {e}\")\n",
    "\n",
    "\n",
    "def save_to_bigquery(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    project_id: str,\n",
    "    if_exists: str = \"append\"\n",
    "):\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"Tidak ada data yang disimpan ke BigQuery untuk tabel {table_name}.\")\n",
    "        return\n",
    "\n",
    "    if not USE_BIGQUERY:\n",
    "        logger.info(\"USE_BIGQUERY=False, skip upload ke BigQuery.\")\n",
    "        return\n",
    "\n",
    "    if \".\" not in table_name:\n",
    "        logger.error(\n",
    "            f\"Nama tabel BigQuery harus dalam format 'dataset.table', \"\n",
    "            f\"didapat: {table_name}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    dataset, table = table_name.split(\".\", 1)\n",
    "    try:\n",
    "        from pandas_gbq import to_gbq\n",
    "        to_gbq(\n",
    "            df,\n",
    "            destination_table=f\"{dataset}.{table}\",\n",
    "            project_id=project_id,\n",
    "            if_exists=if_exists\n",
    "        )\n",
    "        logger.info(f\"Berhasil simpan {len(df)} baris ke BigQuery tabel {dataset}.{table}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saat menyimpan ke BigQuery ({table_name}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_run(\n",
    "    run_date: datetime,\n",
    "    data_sources: List[Dict],\n",
    "    filter_mode: str = \"monthly_first_day\"\n",
    "):\n",
    "\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Mulai proses running untuk tanggal: {run_date.date()} \"\n",
    "                f\"dengan mode filter='{filter_mode}'\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    engine = get_postgres_engine(POSTGRES_CONN_STR)\n",
    "\n",
    "    for src in data_sources:\n",
    "        name = src.get(\"name\")\n",
    "        path = src.get(\"path\")\n",
    "        fmt = src.get(\"format\", \"parquet\")\n",
    "        date_col = src.get(\"date_column\")\n",
    "        target_table_pg = src.get(\"target_table\")\n",
    "\n",
    "\n",
    "        logger.info(f\"--- Proses source: {name} ---\")\n",
    "\n",
    "        # 1. Baca file\n",
    "        df_raw = read_source_file(path, fmt)\n",
    "        if df_raw is None:\n",
    "            logger.error(f\"[{name}] Gagal baca file, skip source ini.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Validasi dataframe\n",
    "        df_valid = validate_dataframe(df_raw, required_columns=[date_col], source_name=name)\n",
    "        if df_valid is None:\n",
    "            logger.error(f\"[{name}] Validasi gagal, skip source ini.\")\n",
    "            continue\n",
    "\n",
    "        # 3. Filter berdasarkan periode\n",
    "        try:\n",
    "            df_filtered = filter_by_period(df_valid, date_col=date_col, run_date=run_date, mode=filter_mode)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{name}] Error saat filter_by_period: {e}\")\n",
    "            continue\n",
    "\n",
    "        if df_filtered is None or df_filtered.empty:\n",
    "            logger.warning(f\"[{name}] Tidak ada data setelah filter, tidak akan disimpan.\")\n",
    "            continue\n",
    "\n",
    "        # 4. Simpan ke PostgreSQL\n",
    "        save_to_postgres(df_filtered, engine=engine, table_name=target_table_pg)\n",
    "\n",
    "    logger.info(f\"Selesai proses running untuk tanggal: {run_date.date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d715707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:41:42,354 [INFO] data_extraction_pipeline - Scheduled run date: 2024-01-01\n",
      "2025-11-16 13:41:42,357 [INFO] data_extraction_pipeline - Scheduled run date: 2024-02-01\n",
      "2025-11-16 13:41:42,358 [INFO] data_extraction_pipeline - Scheduled run date: 2024-03-01\n",
      "2025-11-16 13:41:42,363 [INFO] data_extraction_pipeline - Scheduled run date: 2024-04-01\n",
      "2025-11-16 13:41:42,365 [INFO] data_extraction_pipeline - Scheduled run date: 2024-05-01\n",
      "2025-11-16 13:41:42,367 [INFO] data_extraction_pipeline - Scheduled run date: 2024-06-01\n",
      "2025-11-16 13:41:42,368 [INFO] data_extraction_pipeline - Scheduled run date: 2024-07-01\n"
     ]
    }
   ],
   "source": [
    "def generate_first_day_months(\n",
    "    start_year: int,\n",
    "    start_month: int,\n",
    "    num_months: int\n",
    ") -> List[datetime]:\n",
    "    dates = []\n",
    "    year = start_year\n",
    "    month = start_month\n",
    "\n",
    "    for _ in range(num_months):\n",
    "        dates.append(datetime(year, month, 1))\n",
    "\n",
    "        # Increment bulan\n",
    "        month += 1\n",
    "        if month > 12:\n",
    "            month = 1\n",
    "            year += 1\n",
    "\n",
    "    return dates\n",
    "\n",
    "\n",
    "# Contoh: 7x run dari 1 Jan 2024 s/d 1 Jul 2024\n",
    "run_dates = generate_first_day_months(start_year=2024, start_month=1, num_months=7)\n",
    "for d in run_dates:\n",
    "    logger.info(f\"Scheduled run date: {d.date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617feea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:41:42,389 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:42,390 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-01-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:41:42,392 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:42,674 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:41:42,675 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:41:43,546 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:41:47,529 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-01-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:41:47,624 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:47,625 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:41:47,647 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:41:47,723 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-01-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:41:47,726 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:47,728 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-01-01\n",
      "2025-11-16 13:41:47,729 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:47,732 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-02-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:41:47,733 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:47,872 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:41:47,874 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:41:48,580 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:41:50,743 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-02-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:41:50,858 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:50,860 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:41:50,907 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:41:50,991 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-02-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:41:50,994 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:50,995 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-02-01\n",
      "2025-11-16 13:41:50,996 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:50,999 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-03-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:41:51,002 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:51,167 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:41:51,169 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:41:51,924 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:41:53,334 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-03-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:41:53,423 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:53,425 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:41:53,445 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:41:53,507 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-03-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:41:53,509 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:53,510 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-03-01\n",
      "2025-11-16 13:41:53,512 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:53,515 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-04-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:41:53,516 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:53,636 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:41:53,637 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:41:54,193 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:41:55,358 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-04-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:41:55,438 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:55,440 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:41:55,465 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:41:55,532 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-04-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:41:55,535 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:55,536 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-04-01\n",
      "2025-11-16 13:41:55,537 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:55,539 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-05-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:41:55,540 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:55,635 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:41:55,635 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:41:56,130 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:41:57,986 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-05-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:41:58,091 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:58,116 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:41:58,142 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:41:58,228 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-05-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:41:58,231 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:41:58,234 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-05-01\n",
      "2025-11-16 13:41:58,237 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:58,239 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-06-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:41:58,241 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:41:58,354 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:41:58,356 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:41:59,107 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:42:00,472 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-06-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:42:00,562 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:42:00,566 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:42:00,592 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:42:00,669 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-06-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:42:00,671 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:42:00,673 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-06-01\n",
      "2025-11-16 13:42:00,675 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:42:00,677 [INFO] data_extraction_pipeline - Mulai proses running untuk tanggal: 2024-07-01 dengan mode filter='monthly_first_day'\n",
      "2025-11-16 13:42:00,678 [INFO] data_extraction_pipeline - ================================================================================\n",
      "2025-11-16 13:42:00,792 [INFO] data_extraction_pipeline - Koneksi ke PostgreSQL berhasil.\n",
      "2025-11-16 13:42:00,794 [INFO] data_extraction_pipeline - --- Proses source: yellow_trip ---\n",
      "2025-11-16 13:42:01,364 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet dengan 4251015 baris.\n",
      "2025-11-16 13:42:02,487 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-07-01 menghasilkan 0 baris dari 4251015 baris.\n",
      "2025-11-16 13:42:02,581 [WARNING] data_extraction_pipeline - [yellow_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:42:02,583 [INFO] data_extraction_pipeline - --- Proses source: green_trip ---\n",
      "2025-11-16 13:42:02,605 [INFO] data_extraction_pipeline - Berhasil baca file C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet dengan 48893 baris.\n",
      "2025-11-16 13:42:02,656 [INFO] data_extraction_pipeline - Filter mode='monthly_first_day' untuk tanggal run 2024-07-01 menghasilkan 0 baris dari 48893 baris.\n",
      "2025-11-16 13:42:02,659 [WARNING] data_extraction_pipeline - [green_trip] Tidak ada data setelah filter, tidak akan disimpan.\n",
      "2025-11-16 13:42:02,661 [INFO] data_extraction_pipeline - Selesai proses running untuk tanggal: 2024-07-01\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Mode filter bisa kamu ganti: 'daily', 'weekly', 'monthly', 'monthly_first_day', 'all'\n",
    "    FILTER_MODE = \"monthly_first_day\"\n",
    "\n",
    "    for run_date in run_dates:\n",
    "        try:\n",
    "            process_single_run(\n",
    "                run_date=run_date,\n",
    "                data_sources=DATA_SOURCES,\n",
    "                filter_mode=FILTER_MODE\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Error handling global per running\n",
    "            logger.error(f\"Terjadi error fatal pada run tanggal {run_date.date()}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2840f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:44:14,932 [INFO] data_extraction_pipeline - [yellow] 1145305 baris dihapus karena tidak lolos aturan data cleansing.\n",
      "2025-11-16 13:44:14,935 [INFO] data_extraction_pipeline - [yellow] Data siap pakai: 3105710 baris.\n",
      "2025-11-16 13:44:16,662 [INFO] data_extraction_pipeline - [green] 5531 baris dihapus karena tidak lolos aturan data cleansing.\n",
      "2025-11-16 13:44:16,664 [INFO] data_extraction_pipeline - [green] Data siap pakai: 43362 baris.\n",
      "2025-11-16 13:44:18,730 [INFO] data_extraction_pipeline - Total gabungan df_trips: 3149072 baris dengan kolom: ['VendorID', 'pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee', 'service_type', 'pickup_date', 'pickup_date_str', 'pickup_hour', 'pickup_dow', 'ehail_fee', 'trip_type']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pytz\n",
    "\n",
    "def prepare_single_trip_df(df: pd.DataFrame, service_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Validasi dasar dataframe\n",
    "    - Menyatukan nama kolom pickup/dropoff\n",
    "    - Konversi ke datetime\n",
    "    - Standardisasi timezone (America/New_York)\n",
    "    - Data cleansing dasar\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        logger.error(f\"[{service_type}] Dataframe adalah None.\")\n",
    "        return pd.DataFrame()\n",
    "    if df.empty:\n",
    "        logger.error(f\"[{service_type}] Dataframe kosong.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"service_type\"] = service_type\n",
    "\n",
    "    # Cari kolom pickup/dropoff\n",
    "    pickup_candidates = [\"tpep_pickup_datetime\", \"lpep_pickup_datetime\", \"pickup_datetime\"]\n",
    "    dropoff_candidates = [\"tpep_dropoff_datetime\", \"lpep_dropoff_datetime\", \"dropoff_datetime\"]\n",
    "\n",
    "    pickup_col = next((c for c in pickup_candidates if c in df.columns), None)\n",
    "    dropoff_col = next((c for c in dropoff_candidates if c in df.columns), None)\n",
    "\n",
    "    if pickup_col is None or dropoff_col is None:\n",
    "        logger.error(\n",
    "            f\"[{service_type}] Tidak menemukan kolom pickup/dropoff yang valid. \"\n",
    "            f\"Pickup candidates: {pickup_candidates}, dropoff candidates: {dropoff_candidates}\"\n",
    "        )\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Standarisasi nama kolom\n",
    "    df = df.rename(columns={pickup_col: \"pickup_datetime\", dropoff_col: \"dropoff_datetime\"})\n",
    "\n",
    "    # Konversi ke datetime\n",
    "    for col in [\"pickup_datetime\", \"dropoff_datetime\"]:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "    after = len(df)\n",
    "    if after < before:\n",
    "        logger.warning(f\"[{service_type}] {before - after} baris dihapus karena gagal konversi datetime.\")\n",
    "\n",
    "    # --- Standardisasi timezone ---\n",
    "    # Asumsi: data taxi NYC, berada pada timezone New York.\n",
    "    # Kita standardisasikan ke 'America/New_York' agar konsisten.\n",
    "    ny_tz = pytz.timezone(\"America/New_York\")\n",
    "\n",
    "    for col in [\"pickup_datetime\", \"dropoff_datetime\"]:\n",
    "        if df[col].dt.tz is None:\n",
    "            df[col] = df[col].dt.tz_localize(ny_tz, ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "        else:\n",
    "            df[col] = df[col].dt.tz_convert(ny_tz)\n",
    "\n",
    "    # Drop baris yang gagal karena isu DST\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"pickup_datetime\", \"dropoff_datetime\"])\n",
    "    after = len(df)\n",
    "    if after < before:\n",
    "        logger.warning(f\"[{service_type}] {before - after} baris dihapus karena isu DST.\")\n",
    "\n",
    "    # Kolom turunan tanggal & waktu\n",
    "    df[\"pickup_date\"] = df[\"pickup_datetime\"].dt.date\n",
    "\n",
    "    # Format tanggal diseragamkan ke string \"YYYY-MM-DD\"\n",
    "    df[\"pickup_date_str\"] = df[\"pickup_datetime\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df[\"pickup_hour\"] = df[\"pickup_datetime\"].dt.hour\n",
    "    df[\"pickup_dow\"] = df[\"pickup_datetime\"].dt.dayofweek  # 0=Senin, 6=Minggu\n",
    "\n",
    "    # --- Data cleansing dasar ---\n",
    "    before_clean = len(df)\n",
    "\n",
    "    if \"trip_distance\" in df.columns:\n",
    "        df = df[df[\"trip_distance\"].fillna(0) >= 0]\n",
    "        df = df[df[\"trip_distance\"] <= 200]  # buang outlier sangat jauh\n",
    "\n",
    "    if \"passenger_count\" in df.columns:\n",
    "        df = df[df[\"passenger_count\"].fillna(0) >= 0]\n",
    "        df = df[df[\"passenger_count\"] <= 8]\n",
    "\n",
    "    if \"fare_amount\" in df.columns:\n",
    "        df = df[df[\"fare_amount\"].fillna(0) >= 0]\n",
    "\n",
    "    # Drop trip yang waktunya terbalik\n",
    "    df = df[df[\"dropoff_datetime\"] >= df[\"pickup_datetime\"]]\n",
    "\n",
    "    after_clean = len(df)\n",
    "    if after_clean < before_clean:\n",
    "        logger.info(\n",
    "            f\"[{service_type}] {before_clean - after_clean} baris dihapus \"\n",
    "            \"karena tidak lolos aturan data cleansing.\"\n",
    "        )\n",
    "\n",
    "    logger.info(f\"[{service_type}] Data siap pakai: {len(df)} baris.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Terapkan ke df_yellow dan df_green (hasil cell sebelumnya)\n",
    "df_yellow_clean = prepare_single_trip_df(df_yellow, \"yellow\")\n",
    "df_green_clean  = prepare_single_trip_df(df_green, \"green\")\n",
    "\n",
    "# Gabungkan\n",
    "frames = [df for df in [df_yellow_clean, df_green_clean] if not df.empty]\n",
    "if not frames:\n",
    "    logger.error(\"Tidak ada data valid setelah proses cleansing. df_trips akan kosong.\")\n",
    "    df_trips = pd.DataFrame()\n",
    "else:\n",
    "    df_trips = pd.concat(frames, ignore_index=True)\n",
    "    logger.info(f\"Total gabungan df_trips: {len(df_trips)} baris dengan kolom: {list(df_trips.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a077527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:44:22,625 [INFO] data_extraction_pipeline - Berhasil membuat 5 agregasi.\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def build_aggregations(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Membangun minimal 5 agregasi dari df_trips.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        logger.error(\"Dataframe untuk agregasi kosong.\")\n",
    "        return {}\n",
    "\n",
    "    aggs: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # 1) Total trip per hari\n",
    "    try:\n",
    "        agg1 = (\n",
    "            df.groupby(\"pickup_date_str\")\n",
    "              .size()\n",
    "              .reset_index(name=\"total_trip\")\n",
    "              .sort_values(\"pickup_date_str\")\n",
    "        )\n",
    "        aggs[\"agg1_total_trip_per_hari\"] = agg1\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal membuat agg1_total_trip_per_hari: {e}\")\n",
    "\n",
    "    # 2) Total trip per hari per service_type\n",
    "    try:\n",
    "        agg2 = (\n",
    "            df.groupby([\"pickup_date_str\", \"service_type\"])\n",
    "              .size()\n",
    "              .reset_index(name=\"total_trip\")\n",
    "              .sort_values([\"pickup_date_str\", \"service_type\"])\n",
    "        )\n",
    "        aggs[\"agg2_total_trip_per_hari_per_service\"] = agg2\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal membuat agg2_total_trip_per_hari_per_service: {e}\")\n",
    "\n",
    "    # 3) Rata-rata jarak perjalanan per hari\n",
    "    if \"trip_distance\" in df.columns:\n",
    "        try:\n",
    "            agg3 = (\n",
    "                df.groupby(\"pickup_date_str\")[\"trip_distance\"]\n",
    "                  .agg(avg_trip_distance_km=\"mean\")\n",
    "                  .reset_index()\n",
    "                  .sort_values(\"pickup_date_str\")\n",
    "            )\n",
    "            aggs[\"agg3_rata2_jarak_per_hari\"] = agg3\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal membuat agg3_rata2_jarak_per_hari: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"Kolom trip_distance tidak tersedia, agg3 dilewati.\")\n",
    "\n",
    "    # 4) Total revenue per hari\n",
    "    revenue_cols = [\n",
    "        \"fare_amount\",\n",
    "        \"extra\",\n",
    "        \"mta_tax\",\n",
    "        \"tip_amount\",\n",
    "        \"tolls_amount\",\n",
    "        \"improvement_surcharge\",\n",
    "        \"congestion_surcharge\",\n",
    "        \"total_amount\"\n",
    "    ]\n",
    "    actual_rev_cols = [c for c in revenue_cols if c in df.columns]\n",
    "\n",
    "    if actual_rev_cols:\n",
    "        try:\n",
    "            df_rev = df.copy()\n",
    "            if \"total_amount\" in actual_rev_cols:\n",
    "                df_rev[\"revenue_total\"] = df_rev[\"total_amount\"]\n",
    "            else:\n",
    "                df_rev[\"revenue_total\"] = df_rev[actual_rev_cols].sum(axis=1)\n",
    "\n",
    "            agg4 = (\n",
    "                df_rev.groupby(\"pickup_date_str\")[\"revenue_total\"]\n",
    "                      .agg(total_revenue=\"sum\", avg_revenue_per_trip=\"mean\")\n",
    "                      .reset_index()\n",
    "                      .sort_values(\"pickup_date_str\")\n",
    "            )\n",
    "            aggs[\"agg4_revenue_per_hari\"] = agg4\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal membuat agg4_revenue_per_hari: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"Kolom revenue tidak ditemukan, agg4 dilewati.\")\n",
    "\n",
    "    # 5) Rata-rata jumlah penumpang per hari\n",
    "    if \"passenger_count\" in df.columns:\n",
    "        try:\n",
    "            agg5 = (\n",
    "                df.groupby(\"pickup_date_str\")[\"passenger_count\"]\n",
    "                  .agg(avg_passenger_per_trip=\"mean\")\n",
    "                  .reset_index()\n",
    "                  .sort_values(\"pickup_date_str\")\n",
    "            )\n",
    "            aggs[\"agg5_rata2_penumpang_per_hari\"] = agg5\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal membuat agg5_rata2_penumpang_per_hari: {e}\")\n",
    "    else:\n",
    "        logger.warning(\"Kolom passenger_count tidak tersedia, agg5 dilewati.\")\n",
    "\n",
    "    if len(aggs) < 5:\n",
    "        logger.warning(f\"Hanya {len(aggs)} agregasi yang berhasil dibuat. Cek kembali kolom pada df_trips.\")\n",
    "\n",
    "    logger.info(f\"Berhasil membuat {len(aggs)} agregasi.\")\n",
    "    return aggs\n",
    "\n",
    "\n",
    "aggregations = build_aggregations(df_trips)\n",
    "\n",
    "aggregation_descriptions = {\n",
    "    \"agg1_total_trip_per_hari\": (\n",
    "        \"Agregasi ini menunjukkan jumlah perjalanan taksi per tanggal penjemputan \"\n",
    "        \"(pickup_date). Data ini digunakan untuk melihat tren volume perjalanan \"\n",
    "        \"harian secara keseluruhan.\"\n",
    "    ),\n",
    "    \"agg2_total_trip_per_hari_per_service\": (\n",
    "        \"Agregasi ini membagi jumlah perjalanan per hari berdasarkan jenis layanan \"\n",
    "        \"(yellow/green) sehingga memudahkan analisis kontribusi masing-masing layanan \"\n",
    "        \"pada setiap hari.\"\n",
    "    ),\n",
    "    \"agg3_rata2_jarak_per_hari\": (\n",
    "        \"Agregasi ini menampilkan rata-rata jarak tempuh perjalanan per hari. \"\n",
    "        \"Informasi ini bermanfaat untuk memahami karakteristik jarak perjalanan \"\n",
    "        \"pada masing-masing tanggal.\"\n",
    "    ),\n",
    "    \"agg4_revenue_per_hari\": (\n",
    "        \"Agregasi ini merangkum total pendapatan dan rata-rata pendapatan per trip \"\n",
    "        \"pada setiap hari, berdasarkan kolom tarif yang tersedia di dataset.\"\n",
    "    ),\n",
    "    \"agg5_rata2_penumpang_per_hari\": (\n",
    "        \"Agregasi ini menunjukkan rata-rata jumlah penumpang per perjalanan \"\n",
    "        \"pada setiap hari, sehingga dapat menggambarkan kepadatan penumpang per trip.\"\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "12d0eba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:44:22,849 [ERROR] data_extraction_pipeline - Gagal menyimpan agregasi agg1_total_trip_per_hari ke CSV: [Errno 13] Permission denied: 'output_aggregations\\\\agg1_total_trip_per_hari.csv'\n",
      "2025-11-16 13:44:22,866 [INFO] data_extraction_pipeline - Berhasil menyimpan agg2_total_trip_per_hari_per_service ke output_aggregations\\agg2_total_trip_per_hari_per_service.csv.\n",
      "2025-11-16 13:44:22,881 [INFO] data_extraction_pipeline - Berhasil menyimpan agg3_rata2_jarak_per_hari ke output_aggregations\\agg3_rata2_jarak_per_hari.csv.\n",
      "2025-11-16 13:44:22,886 [INFO] data_extraction_pipeline - Berhasil menyimpan agg4_revenue_per_hari ke output_aggregations\\agg4_revenue_per_hari.csv.\n",
      "2025-11-16 13:44:22,892 [INFO] data_extraction_pipeline - Berhasil menyimpan agg5_rata2_penumpang_per_hari ke output_aggregations\\agg5_rata2_penumpang_per_hari.csv.\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 3: Simpan hasil agregasi ke file CSV\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def save_aggregations_to_csv(\n",
    "    aggs: Dict[str, pd.DataFrame],\n",
    "    output_dir: str = \"output_aggregations\"\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Simpan setiap dataframe agregasi ke file CSV.\n",
    "    Return: {nama_agregasi: path_csv}\n",
    "    \"\"\"\n",
    "    output_paths: Dict[str, str] = {}\n",
    "\n",
    "    if not aggs:\n",
    "        logger.error(\"Tidak ada agregasi yang akan disimpan ke CSV.\")\n",
    "        return output_paths\n",
    "\n",
    "    try:\n",
    "        out_dir = Path(output_dir)\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal membuat direktori '{output_dir}': {e}\")\n",
    "        return output_paths\n",
    "\n",
    "    for name, df in aggs.items():\n",
    "        if df is None or df.empty:\n",
    "            logger.warning(f\"Agregasi {name} kosong, dilewati.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            file_path = out_dir / f\"{name}.csv\"\n",
    "            df.to_csv(file_path, index=False)\n",
    "            output_paths[name] = str(file_path)\n",
    "            logger.info(f\"Berhasil menyimpan {name} ke {file_path}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gagal menyimpan agregasi {name} ke CSV: {e}\")\n",
    "\n",
    "    return output_paths\n",
    "\n",
    "\n",
    "csv_paths = save_aggregations_to_csv(aggregations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
