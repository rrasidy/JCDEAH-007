{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd85bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b485c565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee']\n",
      "['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', 'RatecodeID', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge', 'cbd_congestion_fee']\n"
     ]
    }
   ],
   "source": [
    "df_yellow = pd.read_parquet(r\"C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet\")\n",
    "print(df_yellow.columns.tolist())\n",
    "\n",
    "df_green = pd.read_parquet(r\"C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet\")\n",
    "print(df_green.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b282c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_LEVEL = logging.INFO\n",
    "logging.basicConfig(\n",
    "    level=LOG_LEVEL,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"data_extraction_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Konfigurasi sumber data (2 data source contoh) ---\n",
    "\n",
    "DATA_SOURCES = [\n",
    "    {\n",
    "        \"name\": \"yellow_trip\",\n",
    "        \"path\": r\"C:\\Users\\Robert\\Documents\\bootcamp\\yellow_tripdata_2025-09.parquet\",  # ganti dengan path sebenarnya\n",
    "        \"format\": \"parquet\",                        # atau \"csv\"\n",
    "        \"date_column\": \"tpep_pickup_datetime\",           # ganti sesuai kolom di file\n",
    "        \"target_table\": \"public.yellow_trip\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"green_trip\",\n",
    "        \"path\": r\"C:\\Users\\Robert\\Documents\\bootcamp\\green_tripdata_2025-09.parquet\",   # ganti dengan path sebenarnya\n",
    "        \"format\": \"parquet\",\n",
    "        \"date_column\": \"lpep_pickup_datetime\",           # ganti sesuai kolom di file\n",
    "        \"target_table\": \"public.green_trip\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Konfigurasi koneksi PostgreSQL (contoh) ---\n",
    "# Format: postgresql://username:password@host:port/database\n",
    "POSTGRES_CONN_STR = os.getenv(\n",
    "    \"POSTGRES_CONN_STR\",\n",
    "    \"postgresql://postgres:obet@127.0.0.1:5432/my_database\"  # ganti\n",
    ")\n",
    "\n",
    "USE_BIGQUERY = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d4171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def get_postgres_engine(conn_str: str):\n",
    "    try:\n",
    "        engine = create_engine(conn_str)\n",
    "        # Tes koneksi ringan\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        logger.info(\"Koneksi ke PostgreSQL berhasil.\")\n",
    "        return engine\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(f\"Gagal konek ke PostgreSQL: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_source_file(path: str, file_format: str) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path):\n",
    "        logger.error(f\"File tidak ditemukan: {path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if file_format.lower() == \"parquet\":\n",
    "            df = pd.read_parquet(path)\n",
    "        elif file_format.lower() == \"csv\":\n",
    "            df = pd.read_csv(path)\n",
    "        else:\n",
    "            logger.error(f\"Format file tidak didukung: {file_format}\")\n",
    "            return None\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            logger.warning(f\"Dataframe kosong dari file: {path}\")\n",
    "            return None\n",
    "\n",
    "        logger.info(f\"Berhasil baca file {path} dengan {len(df)} baris.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saat membaca file {path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56644ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataframe(\n",
    "    df: Optional[pd.DataFrame],\n",
    "    required_columns: List[str],\n",
    "    source_name: str\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Validasi dasar dataframe.\"\"\"\n",
    "    if df is None:\n",
    "        logger.error(f\"[{source_name}] Dataframe adalah None.\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        logger.error(f\"[{source_name}] Dataframe kosong.\")\n",
    "        return None\n",
    "\n",
    "    missing_cols = [c for c in required_columns if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        logger.error(f\"[{source_name}] Kolom hilang: {missing_cols}\")\n",
    "        return None\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_by_period(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    run_date: datetime,\n",
    "    mode: str = \"monthly_first_day\"\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    if date_col not in df.columns:\n",
    "        raise ValueError(f\"Kolom tanggal '{date_col}' tidak ditemukan di dataframe.\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Pastikan kolom bertipe datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[date_col])\n",
    "\n",
    "    if mode == \"all\":\n",
    "        return df\n",
    "\n",
    "    if mode == \"daily\":\n",
    "        mask = df[date_col].dt.date == run_date.date()\n",
    "    elif mode == \"weekly\":\n",
    "        mask = (\n",
    "            (df[date_col].dt.isocalendar().year == run_date.isocalendar().year) &\n",
    "            (df[date_col].dt.isocalendar().week == run_date.isocalendar().week)\n",
    "        )\n",
    "    elif mode == \"monthly\":\n",
    "        mask = (\n",
    "            (df[date_col].dt.year == run_date.year) &\n",
    "            (df[date_col].dt.month == run_date.month)\n",
    "        )\n",
    "    elif mode == \"monthly_first_day\":\n",
    "        # Hanya baris yang tanggalnya = 1 di bulan yang sama\n",
    "        mask = (\n",
    "            (df[date_col].dt.year == run_date.year) &\n",
    "            (df[date_col].dt.month == run_date.month) &\n",
    "            (df[date_col].dt.day == 1)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Mode filter tidak dikenal: {mode}\")\n",
    "\n",
    "    filtered = df[mask]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Filter mode='{mode}' untuk tanggal run {run_date.date()} \"\n",
    "        f\"menghasilkan {len(filtered)} baris dari {len(df)} baris.\"\n",
    "    )\n",
    "\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9228c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgres(\n",
    "    df: pd.DataFrame,\n",
    "    engine,\n",
    "    table_name: str,\n",
    "    if_exists: str = \"append\",\n",
    "    chunksize: int = 10_000\n",
    "):\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"Tidak ada data yang disimpan ke PostgreSQL untuk tabel {table_name}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            name=table_name.split(\".\")[-1],    # jika \"schema.table\"\n",
    "            schema=table_name.split(\".\")[0] if \".\" in table_name else None,\n",
    "            con=engine,\n",
    "            if_exists=if_exists,\n",
    "            index=False,\n",
    "            chunksize=chunksize\n",
    "        )\n",
    "        logger.info(f\"Berhasil simpan {len(df)} baris ke PostgreSQL tabel {table_name}.\")\n",
    "    except SQLAlchemyError as e:\n",
    "        logger.error(f\"Error saat menyimpan ke PostgreSQL ({table_name}): {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error umum saat menyimpan ke PostgreSQL ({table_name}): {e}\")\n",
    "\n",
    "\n",
    "def save_to_bigquery(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    project_id: str,\n",
    "    if_exists: str = \"append\"\n",
    "):\n",
    "    if df is None or df.empty:\n",
    "        logger.warning(f\"Tidak ada data yang disimpan ke BigQuery untuk tabel {table_name}.\")\n",
    "        return\n",
    "\n",
    "    if not USE_BIGQUERY:\n",
    "        logger.info(\"USE_BIGQUERY=False, skip upload ke BigQuery.\")\n",
    "        return\n",
    "\n",
    "    if \".\" not in table_name:\n",
    "        logger.error(\n",
    "            f\"Nama tabel BigQuery harus dalam format 'dataset.table', \"\n",
    "            f\"didapat: {table_name}\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    dataset, table = table_name.split(\".\", 1)\n",
    "    try:\n",
    "        from pandas_gbq import to_gbq\n",
    "        to_gbq(\n",
    "            df,\n",
    "            destination_table=f\"{dataset}.{table}\",\n",
    "            project_id=project_id,\n",
    "            if_exists=if_exists\n",
    "        )\n",
    "        logger.info(f\"Berhasil simpan {len(df)} baris ke BigQuery tabel {dataset}.{table}.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saat menyimpan ke BigQuery ({table_name}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_run(\n",
    "    run_date: datetime,\n",
    "    data_sources: List[Dict],\n",
    "    filter_mode: str = \"monthly_first_day\"\n",
    "):\n",
    "\n",
    "    logger.info(\"=\" * 80)\n",
    "    logger.info(f\"Mulai proses running untuk tanggal: {run_date.date()} \"\n",
    "                f\"dengan mode filter='{filter_mode}'\")\n",
    "    logger.info(\"=\" * 80)\n",
    "\n",
    "    engine = get_postgres_engine(POSTGRES_CONN_STR)\n",
    "\n",
    "    for src in data_sources:\n",
    "        name = src.get(\"name\")\n",
    "        path = src.get(\"path\")\n",
    "        fmt = src.get(\"format\", \"parquet\")\n",
    "        date_col = src.get(\"date_column\")\n",
    "        target_table_pg = src.get(\"target_table\")\n",
    "\n",
    "\n",
    "        logger.info(f\"--- Proses source: {name} ---\")\n",
    "\n",
    "        df_raw = read_source_file(path, fmt)\n",
    "        if df_raw is None:\n",
    "            logger.error(f\"[{name}] Gagal baca file, skip source ini.\")\n",
    "            continue\n",
    "\n",
    "        df_valid = validate_dataframe(df_raw, required_columns=[date_col], source_name=name)\n",
    "        if df_valid is None:\n",
    "            logger.error(f\"[{name}] Validasi gagal, skip source ini.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df_filtered = filter_by_period(df_valid, date_col=date_col, run_date=run_date, mode=filter_mode)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{name}] Error saat filter_by_period: {e}\")\n",
    "            continue\n",
    "\n",
    "        if df_filtered is None or df_filtered.empty:\n",
    "            logger.warning(f\"[{name}] Tidak ada data setelah filter, tidak akan disimpan.\")\n",
    "            continue\n",
    "\n",
    "        save_to_postgres(df_filtered, engine=engine, table_name=target_table_pg)\n",
    "\n",
    "    logger.info(f\"Selesai proses running untuk tanggal: {run_date.date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_day_months(\n",
    "    start_year: int,\n",
    "    start_month: int,\n",
    "    num_months: int\n",
    ") -> List[datetime]:\n",
    "    dates = []\n",
    "    year = start_year\n",
    "    month = start_month\n",
    "\n",
    "    for _ in range(num_months):\n",
    "        dates.append(datetime(year, month, 1))\n",
    "\n",
    "        month += 1\n",
    "        if month > 12:\n",
    "            month = 1\n",
    "            year += 1\n",
    "\n",
    "    return dates\n",
    "\n",
    "\n",
    "run_dates = generate_first_day_months(start_year=2024, start_month=1, num_months=7)\n",
    "for d in run_dates:\n",
    "    logger.info(f\"Scheduled run date: {d.date()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
